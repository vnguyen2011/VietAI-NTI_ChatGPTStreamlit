{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vnguyen2011/VietAI-NTI_ChatGPTStreamlit/blob/main/%5BVietAI%5D_Track_1_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Project - S·ª≠ d·ª•ng OpenAI trong th·ª±c t·∫ø & Fine-tune m√¥ h√¨nh\n",
        "B√†i t·∫≠p n√†y t·ªïng bao g·ªìm 200 ƒëi·ªÉm, v·ªõi 2 sections:\n",
        "- Section 1 bao g·ªìm TODO 1, TODO 2: 100 t·ªïng ƒëi·ªÉm\n",
        "- Section 2 bao g·ªìm TODO 3, TODO 4: 100 ƒëi·ªÉm t·ªïng\n",
        "\n",
        "This exercise has 200 points total, consists of 2 sections:\n",
        "- Section 1 includes TODO 1, TODO 2: Total of 100 points.\n",
        "- Section 2 encompasses TODO 3, TODO 4: Total of 100 points."
      ],
      "metadata": {
        "id": "wucQHISSnCP5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. C√†i ƒë·∫∑t v√† import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt"
      ],
      "metadata": {
        "id": "NabxsUmOoAk7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ZOaRKmzcW9n",
        "outputId": "7d7d2787-35ba-4ad4-84f6-5b06b47b9aa3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets\n",
            "  Downloading datasets-2.14.6-py3-none-any.whl (493 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m493.7/493.7 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.6)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Collecting huggingface-hub<1.0.0,>=0.14.0 (from datasets)\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (3.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: dill, multiprocess, huggingface-hub, openai, datasets\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.14.6 dill-0.3.7 huggingface-hub-0.18.0 multiprocess-0.70.15 openai-0.28.1\n"
          ]
        }
      ],
      "source": [
        "!pip install openai datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from datasets import load_dataset\n",
        "import os"
      ],
      "metadata": {
        "id": "9d5CH5BQchos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. C·∫•u h√¨nh OpenAI API Key\n",
        "Sau khi ƒëƒÉng k√Ω API key, c√°c b·∫°n ch√©p key gi·ªëng ƒë·ªãnh d·∫°ng sau `sk-L7qQ...DN98XT` v√†o c√°c chu·ªói ƒëang ƒë·ªÉ tr·ªëng b√™n d∆∞·ªõi."
      ],
      "metadata": {
        "id": "Tqb1mQ1huybk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key = \"\" ## To configure OpenAI API\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\" ## To configure langchain connections with OpenAI"
      ],
      "metadata": {
        "id": "kTUFHndpcz9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Tri·ªÉn khai"
      ],
      "metadata": {
        "id": "YOGS69gFCeoh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[ENGLISH BELOW]**\n",
        "\n",
        "**SECTION 1 (100 ƒëi·ªÉm): [TODO 1]**\n",
        "\n",
        "**TODO 1: (40 ƒëi·ªÉm) Chu·∫©n b·ªã Dataset**\n",
        "\n",
        "- Nhi·ªám v·ª• c·ªßa b·∫°n l√† t·ª± t√¨m dataset ph√π h·ª£p cho b√†i to√°n\n",
        " - (20 ƒëi·ªÉm) T√¨m m·ªôt dataset d√πng ƒë·ªÉ ph√¢n lo·∫°i d·ªØ li·ªáu (classification), khuy·∫øn kh√≠ch li√™n quan ƒë·∫øn lƒ©nh v·ª±c l√†m vi·ªác c√° nh√¢n.\n",
        " - (20 ƒëi·ªÉm) Dataset c·ªßa b·∫°n n√™n c√≥ 5+ nh√£n (labels) ph√¢n lo·∫°i.\n",
        "- (0 ƒëi·ªÉm) N·∫øu b·∫°n kh√¥ng t√¨m ƒë∆∞·ª£c dataset ph√π h·ª£p ho·∫∑c mu·ªën s·ª≠ d·ª•ng dataset m·∫´u ƒë∆∞·ª£c cung c·∫•p b·ªüi VietAI:\n",
        " - **Emoji classification**: Ph√¢n lo·∫°i tweets v√†o emoji ph√π h·ª£p nh·∫•t ƒë·ªÉ mi√™u t·∫£ tweet ƒë√≥: https://huggingface.co/datasets/tweet_eval/viewer/emoji/train\n",
        "------------\n",
        "**[VIETNAMESE ABOVE]**\n",
        "\n",
        "**SECTION 1 (100 points): [TODO 1]**\n",
        "\n",
        "**TODO 1: (40 points) Prepare dataset**\n",
        "\n",
        "- Your task is to find the appropriate dataset for the problem\n",
        "  - (20 points) Find a dataset used for data classification (classification), preferably related to personal work field.\n",
        "  - (20 points) Your Dataset should have 5+ classification labels.\n",
        "- (0 points) If you cannot find a suitable dataset or want to use the sample dataset provided by VietAI:\n",
        "  - **Emoji classification**: Classify tweets into the most suitable emoji to describe that tweet: https://huggingface.co/datasets/tweet_eval/viewer/emoji/train\n"
      ],
      "metadata": {
        "id": "JoL8-F1YvdQ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "D∆∞·ªõi ƒë√¢y l√† ƒëo·∫°n code h∆∞·ªõng d·∫´n b·∫°n chu·∫©n b·ªã d·ªØ li·ªáu ƒë∆∞·ª£c VietAI cung c·∫•p. Ch√∫ng ta s·∫Ω s·ª≠ d·ª•ng h√†m `load_dataset` c·ªßa th∆∞ vi·ªán `datasets` c·ªßa Hugging Face"
      ],
      "metadata": {
        "id": "MkWaukfKxzkT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"tweet_eval\"\n",
        "\n",
        "train_dataset = dataset = load_dataset(data_path, \"emoji\", split = \"train\") ## Train data\n",
        "valid_dataset = load_dataset(data_path, \"emoji\", split = \"validation\") ## Valid data\n",
        "test_dataset = load_dataset(data_path, \"emoji\", split = \"test\") ## Test Data"
      ],
      "metadata": {
        "id": "bQ9GeMChe9TD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[ENGLISH BELOW]**\n",
        "\n",
        "Trong h·ªçc m√°y, m·ªôt t·∫≠p d·ªØ li·ªáu th∆∞·ªùng ƒë∆∞·ª£c chia th√†nh ba ph·∫ßn: t·∫≠p hu·∫•n luy·ªán, t·∫≠p ki·ªÉm ƒë·ªãnh, v√† t·∫≠p ki·ªÉm tra. D∆∞·ªõi ƒë√¢y l√† m·ª•c ƒë√≠ch c·ªßa t·ª´ng ph·∫ßn:\n",
        "\n",
        "1. **T·∫≠p Hu·∫•n Luy·ªán (Train Set)**: ƒê√¢y l√† ph·∫ßn l·ªõn nh·∫•t c·ªßa d·ªØ li·ªáu, ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ \"d·∫°y\" m√¥ h√¨nh m√°y h·ªçc. ƒê√≥ gi·ªëng nh∆∞ s√°ch gi√°o khoa d√†nh cho m·ªôt h·ªçc sinh; n√≥ ch·ª©a c√°c v√≠ d·ª• m√† m√¥ h√¨nh h·ªçc h·ªèi t·ª´ ƒë√≥.\n",
        "\n",
        "2. **T·∫≠p Ki·ªÉm ƒê·ªãnh (Validation Set)**: Ph·∫ßn n√†y ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ ƒëi·ªÅu ch·ªânh c√°c tham s·ªë c·ªßa m√¥ h√¨nh v√† ƒë·ªÉ ngƒÉn ch·∫∑n t√¨nh tr·∫°ng qu√° kh·ªõp. Qu√° kh·ªõp l√† khi m√¥ h√¨nh ho·∫°t ƒë·ªông t·ªët tr√™n d·ªØ li·ªáu hu·∫•n luy·ªán nh∆∞ng k√©m tr√™n d·ªØ li·ªáu m·ªõi, ch∆∞a t·ª´ng th·∫•y. T·∫≠p ki·ªÉm ƒë·ªãnh gi·ªëng nh∆∞ m·ªôt b√†i ki·ªÉm tra gi·∫£ ƒë·ªãnh ƒë·ªÉ h·ªçc sinh luy·ªán t·∫≠p tr∆∞·ªõc khi thi cu·ªëi k·ª≥. N√≥ gi√∫p ki·ªÉm tra hi·ªáu su·∫•t trong qu√° tr√¨nh hu·∫•n luy·ªán.\n",
        "\n",
        "3. **T·∫≠p Ki·ªÉm Tra (Test Set)**: ƒê√¢y l√† ph·∫ßn d·ªØ li·ªáu ri√™ng bi·ªát kh√¥ng ƒë∆∞·ª£c m√¥ h√¨nh th·∫•y trong qu√° tr√¨nh hu·∫•n luy·ªán ho·∫∑c ki·ªÉm ƒë·ªãnh. N√≥ ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ ƒë√°nh gi√° hi·ªáu su·∫•t c·ªßa m√¥ h√¨nh sau khi qu√° tr√¨nh hu·∫•n luy·ªán ho√†n t·∫•t. T·∫≠p ki·ªÉm tra gi·ªëng nh∆∞ b√†i thi cu·ªëi c√πng cho h·ªçc sinh, ƒë√°nh gi√° xem m√¥ h√¨nh ƒë√£ h·ªçc t·ªët ƒë·∫øn m·ª©c n√†o.\n",
        "\n",
        "L√Ω do ch√∫ng ta c·∫ßn ph·∫£i chia nh·ªØng ph·∫ßn n√†y l√† ƒë·ªÉ ƒë·∫£m b·∫£o r·∫±ng m√¥ h√¨nh c·ªßa ch√∫ng ta c√≥ kh·∫£ nƒÉng t·ªïng qu√°t h√≥a t·ªët ƒë·ªëi v·ªõi d·ªØ li·ªáu m·ªõi, ch∆∞a t·ª´ng th·∫•y, kh√¥ng ch·ªâ l√† d·ªØ li·ªáu n√≥ ƒë∆∞·ª£c hu·∫•n luy·ªán tr√™n. ƒêi·ªÅu n√†y gi√∫p x√¢y d·ª±ng nh·ªØng m√¥ h√¨nh m·∫°nh m·∫Ω c√≥ kh·∫£ nƒÉng ƒë∆∞a ra d·ª± ƒëo√°n ch√≠nh x√°c trong c√°c t√¨nh hu·ªëng th·ª±c t·∫ø, ngo√†i m√¥i tr∆∞·ªùng hu·∫•n luy·ªán.\n",
        "\n",
        "----------------------\n",
        "**[VIETNAMESE ABOVE]**\n",
        "\n",
        "In machine learning, a dataset is typically split into three parts: training, validation, and test sets. Here's what each of them is for:\n",
        "\n",
        "1. **Training Set**: This is the largest portion of the data, which is used to train the machine learning model. It's like the textbook for a student; it contains examples that the model learns from.\n",
        "\n",
        "2. **Validation Set**: This set is used to tune the model's parameters and to prevent overfitting. Overfitting is when the model performs well on the training data but poorly on new, unseen data. The validation set acts like a mock exam for the student to practice on before the final exam. It helps in checking the performance during the training process.\n",
        "\n",
        "3. **Test Set**: This is a separate portion of data that is not seen by the model during the training or validation phases. It is used to evaluate the model's performance after the training is complete. The test set is like the final exam for the student, assessing how well the model has learned overall.\n",
        "\n",
        "The reason we need these splits is to make sure that our model can generalize well to new, unseen data, not just the data it was trained on. This helps in building robust models that make accurate predictions in real-world situations, outside of the training environment."
      ],
      "metadata": {
        "id": "Wlpdxg62lY7G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNxlMKW_fe1a",
        "outputId": "0575a75b-decb-4102-8a75-113d5bf479b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text', 'label'],\n",
              "    num_rows: 45000\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Length of dataset:\", len(dataset))\n",
        "print(\"Example data: Index 1:\", dataset[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJbUyHW0fk1j",
        "outputId": "8641b9a6-3301-42ab-9ec9-ce94d8bd31d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of dataset: 45000\n",
            "Example data: Index 1: {'text': \"Time for some BBQ and whiskey libations. Chomp, belch, chomp! (@ Lucille's Smokehouse Bar-B-Que)\", 'label': 19}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ƒê√¢y l√† nh√£n d·ªØ li·ªáu m√† ch√∫ng ta s·∫Ω d·ª± ƒëo√°n"
      ],
      "metadata": {
        "id": "TWKwThXVkJ1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset[0][\"label\"]) ## Nh√£n c·ªßa data 0\n",
        "print(dataset[1][\"label\"]) ## Nh√£n c·ªßa data 1\n",
        "print(any(type(x) == int for x in dataset[\"label\"])) ## Ki·ªÉm tra xem nh√£n c√≥ ph·∫£i l√† int hay kh√¥ng"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1FsfJgG3TK7",
        "outputId": "cf6b3520-57ed-40ee-98c5-74d4645428bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12\n",
            "19\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do c√°c `label` ƒë·ªÅu ·ªü d·∫°ng **int**, kh√¥ng c√≥ qu√° nhi·ªÅu √Ω nghƒ©a v·ªõi m√¥ h√¨nh, n√™n ta c√≥ th·ªÉ th√™m ch√∫ th√≠ch r√µ r√†ng cho m·ªói `label`:"
      ],
      "metadata": {
        "id": "ipnXE1UI3k6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels = {\n",
        "    0: \"black heart\",\n",
        "    1: \"smiling face with heart-eyes\",\n",
        "    2: \"face with tears of joy\",\n",
        "    3: \"two heart\",\n",
        "    4: \"fire\",\n",
        "    5: \"smiling face with smiling eyes\",\n",
        "    6: \"smiling face with sunglasses\",\n",
        "    7: \"sparkles\",\n",
        "    8: \"blue heart\",\n",
        "    9: \"face blowing a kiss\",\n",
        "    10: \"camera\",\n",
        "    11: \"United States\",\n",
        "    12: \"sun\",\n",
        "    13: \"purple heart\",\n",
        "    14: \"winking face\",\n",
        "    15: \"hundred point symbol\",\n",
        "    16: \"beaming face with smilig eyes\",\n",
        "    17: \"Christmas tree\",\n",
        "    18: \"camera with flash\",\n",
        "    19: \"winking face with tongue\",\n",
        "    }"
      ],
      "metadata": {
        "id": "KW2ps1_afoc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[ENGLISH BELOW]**\n",
        "\n",
        "**SECTION 1 (100 ƒëi·ªÉm): [TODO 2]**\n",
        "\n",
        "**TODO 2: (60 ƒëi·ªÉm) Gi·∫£i quy·∫øt b√†i to√°n tr√™n d·ªØ li·ªáu ƒë√£ chu·∫©n b·ªã b·∫±ng prompting**\n",
        "\n",
        "- M·ª•c ti√™u: D·ª± ƒëo√°n ch√≠nh x√°c nh√£n d√°n cho dataset\n",
        " - (30 ƒëi·ªÉm) √Åp d·ª•ng k·ªπ thu·∫≠t Zero-shot tr√™n dataset\n",
        " - (30 ƒëi·ªÉm) √Åp d·ª•ng c√°c k·ªπ thu·∫≠t prompting tr√™n dataset (Few-shot, Chain of Thought (CoT))\n",
        "------------\n",
        "**[VIETNAMESE ABOVE]**\n",
        "\n",
        "**SECTION 1 (100 points): [TODO 2]**\n",
        "\n",
        "**TODO 2: (60 points) Solve problems on prepared data using prompting**\n",
        "\n",
        "- Goal: Correctly predict labels for the dataset\n",
        "  - (30 points) Apply Zero-shot technique on dataset\n",
        "  - (30 points) Apply prompting techniques on dataset (Few-shot, Chain of Thought (CoT))\n"
      ],
      "metadata": {
        "id": "N6YMgr-L5z5_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***G·ª£i √Ω:***\n",
        "\n",
        "M·ª•c ti√™u c·ªßa ch√∫ng ta l√† d·ª± ƒëo√°n ch√≠nh x√°c nh√£n cho dataset, v·∫≠y n√™n ta c·∫ßn ph·∫£i t·∫°o m·∫´u prompt ph√π h·ª£p v·ªõi y√™u c·∫ßu. Prompt **c√≥ th·ªÉ** c√≥ c·∫•u tr√∫c nh∆∞ sau:\n",
        "##`prompt = <task desciption> + <instruction> + <examples> + <label>`\n",
        "Trong ƒë√≥:\n",
        "\n",
        "`task description`: m√¥ t·∫£ nhi·ªám v·ª•\n",
        "\n",
        "`instruction`: y√™u c·∫ßu th·ª±c hi·ªán nhi·ªám v·ª•\n",
        "\n",
        "`examples`: v√≠ d·ª• (th∆∞·ªùng s·ª≠ d·ª•ng trong one-shot, few-shot)\n",
        "\n",
        "`label`: danh s√°ch c√°c nh√£n (v√≠ d·ª•: `labels` trong TODO 1)\n"
      ],
      "metadata": {
        "id": "2zrSsSmU7i5d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[ENGLISH BELOW]**\n",
        "\n",
        "**SECTION 2 (100 ƒëi·ªÉm): [TODO 3]**\n",
        "\n",
        "**TODO 3: (60 ƒëi·ªÉm) Gi·∫£i quy·∫øt b√†i to√°n b·∫±ng fine-tuning**\n",
        "\n",
        "- M·ª•c ti√™u: D·ª± ƒëo√°n ch√≠nh x√°c nh√£n d√°n cho dataset\n",
        " - (40 ƒëi·ªÉm) √Åp d·ª•ng kƒ© thu·∫≠t fine-tuning ƒë√£ ƒë∆∞·ª£c h·ªçc v√†o dataset\n",
        " - (20 ƒëi·ªÉm t·ªëi ƒëa) Bonus point:\n",
        "   - (10 ƒëi·ªÉm) Ph√¢n t√≠ch v√† so s√°nh performance gi·ªØa c√°c kƒ© thu·∫≠t kh√°c nhau\n",
        "   - (10 ƒëi·ªÉm) Thay ƒë·ªïi s·ªë l∆∞·ª£ng d·ªØ li·ªáu ƒë·ªÉ fine-tune v√† quan s√°t m·ªëi quan h·ªá gi·ªØa l∆∞·ª£ng d·ªØ li·ªáu v√† model performance\n",
        "------------\n",
        "**[VIETNAMESE ABOVE]**\n",
        "\n",
        "**SECTION 2 (100 points): [TODO 3]**\n",
        "\n",
        "**TODO 3: (60 points) Solve the problem with fine-tuning**\n",
        "\n",
        "- Goal: Correctly predict labels for dataset\n",
        "  - (40 points) Apply the learned fine-tuning technique to the dataset\n",
        "  - (20 points maximum) Bonus point:\n",
        "    - (10 points) Analyze and compare performance between different techniques\n",
        "    - (10 points) Change the amount of data to fine-tune and observe the relationship between the amount of data and model performance\n"
      ],
      "metadata": {
        "id": "oQP576ADpSFc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***G·ª£i √Ω:***\n",
        "\n",
        "- Khi s·ª≠ d·ª•ng d·ªØ li·ªáu ƒë∆∞·ª£c cung c·∫•p c·ªßa VietAI, ƒë·ªÉ l·∫•y m·ªôt subset trong d·ªØ li·ªáu g·ªëc, ta c√≥ th·ªÉ l√†m nh∆∞ sau:\n"
      ],
      "metadata": {
        "id": "TFtHIBEYuu_H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_samples = 1000\n",
        "samples_dataset = dataset.select(range(num_samples))\n",
        "\n",
        "# or\n",
        "samples_dataset = dataset.shuffle(seed = 42).select(range(num_samples))\n",
        "samples_dataset"
      ],
      "metadata": {
        "id": "-ZxojQVuv0id",
        "outputId": "05504378-d69b-46be-f612-f075b6dc5635",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text', 'label'],\n",
              "    num_rows: 1000\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ƒê·ªëi v·ªõi b√†i to√°n classification, ta c√≥ th·ªÉ ƒë√°nh gi√° performance qua c√°c metrics nh∆∞ Accuracy, Precision, Recall ho·∫∑c F1-score\n",
        "\n",
        "- Input:\n",
        " - `y_true`: ƒê√¢y l√† labels th·ª±c t·∫ø - true labels.\n",
        " - `y_pred`: ƒê√¢y l√† labels ƒë∆∞·ª£c d·ª± ƒëo√°n b·ªüi m√¥ h√¨nh - predicted labels.\n",
        "\n",
        " M·ª•c ti√™u c·ªßa ch√∫ng ta l√† l√†m sao `y_pred` g·∫ßn v·ªõi `y_true` nh·∫•t: M√¥ h√¨nh ƒëo√°n r·∫•t ƒë√∫ng. Ch√∫ng ta mu·ªën t·ªëi ∆∞u ho√° `macro avg` F1-Score c·ªßa **`valid_dataset, test_dataset`** ·ªü b·∫£ng sau:"
      ],
      "metadata": {
        "id": "rVgdAS67yN7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "y_true = [1,1,0,2,3,0,1,3,6]\n",
        "y_pred = [1,1,2,2,2,1,1,4,8]\n",
        "\n",
        "report = classification_report(y_true, y_pred)\n",
        "\n",
        "# C·ªôt ƒë·∫ßu ti√™n l√† nh√£n, c√°c c·ªôt sau l·∫ßn l∆∞·ª£t l√† precision, recall, f1-score cho t·ª´ng nh√£n\n",
        "# C·ªôt support l√† s·ªë nh√£n theo t·ª´ng lo·∫°i trong y_true\n",
        "# H√†ng th·ª© 3 t·ª´ d∆∞·ªõi l√™n l√† accuracy (ƒë·ªô ch√≠nh x√°c)\n",
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ge96Z8w7-8YC",
        "outputId": "d64fb724-0cb6-4508-897a-de34c2aec2c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         2\n",
            "           1       0.75      1.00      0.86         3\n",
            "           2       0.33      1.00      0.50         1\n",
            "           3       0.00      0.00      0.00         2\n",
            "           4       0.00      0.00      0.00         0\n",
            "           6       0.00      0.00      0.00         1\n",
            "           8       0.00      0.00      0.00         0\n",
            "\n",
            "    accuracy                           0.44         9\n",
            "   macro avg       0.15      0.29      0.19         9\n",
            "weighted avg       0.29      0.44      0.34         9\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[ENGLISH BELOW]**\n",
        "\n",
        "**SECTION 2 (100 ƒëi·ªÉm): [TODO 4]**\n",
        "\n",
        "**TODO 4 : (40 ƒëi·ªÉm) X√¢y d·ª±ng giao di·ªán (web UI) ƒë·ªÉ t∆∞∆°ng t√°c v·ªõi s·∫£n ph·∫©m**\n",
        "\n",
        "- B·∫°n c√≥ th·ªÉ x√¢y d·ª±ng m·ªôt giao di·ªán ph√π h·ª£p ƒë·ªÉ t∆∞∆°ng t√°c:\n",
        "  - V√≠ d·ª•: 1 giao di·ªán cho ng∆∞·ªùi d√πng upload data v√† nh·∫≠n l·∫°i file nh√£n d√°n d·ª± b√°o (predicted labels)\n",
        "\n",
        "------------\n",
        "**[VIETNAMESE ABOVE]**\n",
        "\n",
        "**SECTION 2 (100 points): [TODO 4]**\n",
        "\n",
        "**TODO 4: (40 points) Build interface (web UI) to interact with the product**\n",
        "\n",
        "- You can build an interface suitable for interaction:\n",
        "  - For example: an interface for users to upload data and receive back a predicted labels file.\n"
      ],
      "metadata": {
        "id": "fqdVDXDNAfkU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *Ch√∫c c√°c b·∫°n l√†m b√†i t·∫≠p th·∫≠t t·ªët ü•∞*"
      ],
      "metadata": {
        "id": "sW5vWik7BPK0"
      }
    }
  ]
}